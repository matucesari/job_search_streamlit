# üì¶ Buscador de Ofertas Laborales LATAM (Streamlit + Docker)

Esta aplicaci√≥n permite realizar b√∫squedas de ofertas laborales en los principales portales de empleo de Am√©rica Latina utilizando scraping con Python. La interfaz est√° construida en Streamlit, y el sistema est√° contenerizado con Docker y orquestado con Redis para caching temporal.

## üåê Portales soportados
- LinkedIn *(requiere navegador Playwright)*
- Computrabajo *(HTML est√°tico)*
- Bumeran *(requiere navegador Playwright)*
- Indeed *(HTML est√°tico)*
- Jooble *(HTML est√°tico)*
- Glassdoor *(requiere navegador Playwright)*

## üß∞ Tecnolog√≠as
- Python 3.10
- Streamlit
- Requests + BeautifulSoup4
- Playwright (Chromium headless)
- Redis (para cachear resultados)
- Docker & Docker Compose

---

## üöÄ Instrucciones de uso

### 1. Clonar el repositorio
```bash
git clone https://github.com/tuusuario/job-search-streamlit.git
cd job-search-streamlit
```

### 2. Construir e iniciar los contenedores
```bash
docker-compose up --build
```

### 3. Acceder a la app
Abre tu navegador en [http://localhost:8501](http://localhost:8501)

---

## üñ•Ô∏è Interfaz de usuario

Desde la interfaz Streamlit puedes:
- Ingresar **palabras clave** (ej: `python`, `data scientist`, etc.)
- Especificar **pa√≠s o regi√≥n**
- Seleccionar uno o m√°s portales
- Elegir cu√°ntas p√°ginas de resultados scrapea por portal
- Incluir o excluir portales que requieren JavaScript (uso de navegador)

Los resultados se muestran en una tabla y pueden descargarse como archivo CSV.

---

## ‚öôÔ∏è Arquitectura interna

- Cada scraper se encuentra en el m√≥dulo `scrapers/` y hereda de `JobScraper`
- Control de **cuota de requests** (10 por minuto por portal)
- Soporte para **rotaci√≥n de proxies** (puedes configurar tu lista en la clase base)
- Uso de **Redis como cach√© en memoria**, purgado al reiniciar
- **Playwright** se usa solo cuando es necesario (portales din√°micos)

---

## üß© Agregar nuevos portales
1. Crear un nuevo archivo en `scrapers/` (ej: `trabajando_scraper.py`)
2. Crear una clase que herede de `JobScraper` e implemente el m√©todo `scrape()`
3. Registrar el nuevo scraper en `streamlit_app.py`

---

## üìå Notas
- El uso de proxies es opcional, pero recomendado para evitar bloqueos
- Este proyecto es solo para fines educativos e investigaci√≥n. Respeta los t√©rminos de servicio de cada portal
- LinkedIn y Glassdoor pueden aplicar restricciones adicionales

---

## üì¨ Contacto
Desarrollado por: **Matilde I C√©sari**  
Repositorio: [github.com/matucesari/job-search-streamlit](https://github.com/matucesari/job-search-streamlit)

## ‚úÖ Fix

1. Error en Indeed y Jooble: problema de dominio punycode "M√©xico"
Causa: El pa√≠s "M√©xico" fue interpretado como parte del subdominio y convertido autom√°ticamente a punycode (xn--mxico-bsa).
Soluci√≥n: En el scraper de Indeed y Jooble, debemos usar el c√≥digo de pa√≠s est√°ndar (por ejemplo, mx) como parte del dominio, no el nombre completo.

Modificar en los scrapers:
<
base_url = f"https://{country.lower()}.indeed.com/jobs"
>
por:

<
country_code = {"argentina": "ar", "mexico": "mx", "chile": "cl", "colombia": "co", ...}
base_url = f"https://{country_code.get(country.lower(), 'www')}.indeed.com/jobs"
>
Y lo mismo para Jooble.

Tambi√©n podr√≠amos aplicar una funci√≥n de normalizaci√≥n de pa√≠s que remapee "M√©xico" ‚Üí "mx", "Argentina" ‚Üí "ar", etc.

 2. Error en Bumeran:
Causa: Est√°s haciendo: for card in soup.select(".aviso a.aviso_link"),:
    card.select_one(...)
pero soup.select(...) ya devuelve una lista, y al agregar , est√°s creando una tupla de una lista, no iterando sobre los elementos. Es un error sutil de sintaxis.
Reemplaz√°:
<
for card in soup.select(".aviso a.aviso_link"),:
>
por:
<
for card in soup.select(".aviso a.aviso_link"):
>

## ‚úÖ Recomendaciones:
Agregar try/except por scraper para capturar excepciones y continuar con los dem√°s portales sin frenar la app.
* Mostrar logs de errores al usuario (como est√°s haciendo).
* Probar con otros t√©rminos (python, data, analista) para descartar que no haya resultados reales.
* Hacer debug con st.write(soup.prettify()[:1000]) para ver si se est√° obteniendo el HTML esperado.


